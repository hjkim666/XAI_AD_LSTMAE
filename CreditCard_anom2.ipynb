{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing some Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import holoviews as hv\n",
    "from holoviews import opts\n",
    "hv.extension('bokeh')\n",
    "\n",
    "import tensorflow as tf\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, LSTM, TimeDistributed, RepeatVector\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lime import lime_tabular\n",
    "from IPython.display import Image \n",
    "import tempfile\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "pd.set_option('display.max_rows', 15)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Read The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read The data\n",
    "df = pd.read_csv('Data/creditcard.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=df[\"Time\"], y=df[\"V1\"], mode='lines', name='V1'))\n",
    "fig.add_trace(go.Scatter(x=df[\"Time\"], y=df[\"V2\"], mode='lines', name='V2'))\n",
    "fig.add_trace(go.Scatter(x=df[\"Time\"], y=df[\"V3\"], mode='lines', name='V3'))\n",
    "fig.update_layout(title_text=\"V1 vs Cyl V2 vs V3\", yaxis1=dict(title=\"Values\", side='left'),\n",
    "                  yaxis2=dict(title=\"\", side='right', anchor=\"x\", overlaying=\"y\")\n",
    "                  )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation\n",
    "dataplot = sns.heatmap(df.corr(), cmap=\"YlGnBu\") \n",
    "# displaying heatmap \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for class distribution\n",
    "sns.countplot(x=\"Class\",data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df[\"Class\"].value_counts()/284807)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(\"Class\",axis=1)\n",
    "y = df[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. LSTM Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split The Data into Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_timestamp = df[['Time']]\n",
    "\n",
    "df_ = df.drop('Time', axis=1)\n",
    "#df_ = df.drop(['Time','Class'], axis=1)\n",
    "df_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prp = .80\n",
    "train = df_.loc[:df_.shape[0] * train_prp]\n",
    "test = df_.loc[df_.shape[0] * train_prp:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize The Data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train)\n",
    "X_test = scaler.transform(test)\n",
    "X_train_origin = X_train\n",
    "X_test_origin = X_test\n",
    "print(\"X train Shape:\", X_train.shape)\n",
    "print(\"X test Shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the Dimension of the Train and Test set for LSTM Model\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "print(\"X train Shape:\", X_train.shape)\n",
    "print(\"X test Shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_model(X):\n",
    "    # Build the LSTM Autoencoder model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Encoder\n",
    "    model.add(LSTM(100, activation='relu', input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "    model.add(LSTM(50, activation='relu', return_sequences=False))\n",
    "    model.add(RepeatVector(X.shape[1]))\n",
    "\n",
    "   # Decoder\n",
    "    model.add(LSTM(50, activation='relu', return_sequences=True))\n",
    "    model.add(LSTM(100, activation='relu', return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(X.shape[2])))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = autoencoder_model(X_train)\n",
    "model.compile(optimizer='adam', loss='mae', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[-1].get_weights()[0].shape   # 100, 30 \n",
    "model.layers[-2].get_weights()[0].shape   # 50, 400\n",
    "model.layers[-3].get_weights()[0].shape   # 50, 200\n",
    "\n",
    "model.layers[0].get_weights()[0].shape   # 30,400\n",
    "model.layers[1].get_weights()[0].shape   # 100,200\n",
    "model.layers[3].get_weights()[0].shape   # 50,200\n",
    "model.layers[4].get_weights()[0].shape   # 50,400\n",
    "model.layers[5].get_weights()[0].shape   # 100,30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "batch = 25\n",
    "history = model.fit(X_train, X_train, epochs=epochs, batch_size=batch, validation_split=.2, verbose=1).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=[x for x in range(len(history['loss']))], y=history['loss'], mode='lines', name='loss'))\n",
    "fig.add_trace(go.Scatter(x=[x for x in range(len(history['val_loss']))], y=history['val_loss'], mode='lines', name='validation loss'))\n",
    "fig.update_layout(title=\"LSTM AE Error Loss Over Epochs\", yaxis=dict(title=\"Loss\"), xaxis=dict(title=\"Epoch\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=[x for x in range(len(history['accuracy']))], y=history['accuracy'], mode='lines', name='accuracy'))\n",
    "fig.add_trace(go.Scatter(x=[x for x in range(len(history['val_accuracy']))], y=history['val_accuracy'], mode='lines', name='validation accuracy'))\n",
    "fig.update_layout(title=\"LSTM AE Accuracy Over Epochs\", yaxis=dict(title=\"Accuracy\"), xaxis=dict(title=\"Epoch\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how loss & mse went down\n",
    "epoch_loss = history['loss']\n",
    "epoch_val_loss = history['val_loss']\n",
    "epoch_mae = history['accuracy']\n",
    "epoch_val_mae = history['val_accuracy']\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(0,len(epoch_loss)), epoch_loss, 'b-', linewidth=2, label='Train Loss')\n",
    "plt.plot(range(0,len(epoch_val_loss)), epoch_val_loss, 'r-', linewidth=2, label='Test Loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "#lt.title('Loss')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('CreditCard_Figure_Loss_LSTM_AE.jpeg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how loss & mse went down\n",
    "epoch_loss = history['loss']\n",
    "epoch_val_loss = history['val_loss']\n",
    "epoch_mae = history['accuracy']\n",
    "epoch_val_mae = history['val_accuracy']\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "plt.plot(range(0,len(epoch_mae)), epoch_mae, 'b-', linewidth=2, label='Train Acc')\n",
    "plt.plot(range(0,len(epoch_val_mae)), epoch_val_mae, 'r-', linewidth=2,label='Test Acc')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "#plt.title('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.savefig('CreditCard_Figure_Acc_LSTM_AE.jpeg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and architecture to single file\n",
    "model.save('CreditCard_LSTM_AE_Model.h5')\n",
    "print(\"Model Saved to a Disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = model.predict(X_train)\n",
    "X_pred = X_pred.reshape(X_pred.shape[0], X_pred.shape[2])\n",
    "X_pred = scaler.inverse_transform(X_pred)\n",
    "X_pred = pd.DataFrame(X_pred, columns=train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred2 = model.predict(X_test)\n",
    "\n",
    "# Compute loss/error between predicted and test values columns\n",
    "mse_loss = np.mean(np.square(X_pred2 - X_test))  # MSE loss\n",
    "mae_loss = np.mean(np.abs(X_pred2 - X_test))  # Mean absolute error (MAE) loss\n",
    "\n",
    "print(f\"MSE Loss: {mse_loss}\")\n",
    "print(f\"MAE Loss: {mae_loss}\")\n",
    "X_pred_scaled = X_pred2.reshape(X_pred2.shape[0], X_pred2.shape[2])\n",
    "X_pred2 = scaler.inverse_transform(X_pred_scaled)\n",
    "X_pred2 = pd.DataFrame(X_pred2, columns=train.columns)\n",
    "X_pred_df2 = pd.DataFrame(X_pred2, columns=train.columns)\n",
    "X_pred2.index = test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_ = pd.DataFrame()\n",
    "scores_ = pd.concat([X_pred, X_pred2])\n",
    "scores_\n",
    "scores_['datetime'] = df_timestamp\n",
    "\n",
    "reconstruction_errors = np.mean(np.abs(pd.concat([X_pred, X_pred2]) - pd.concat([train, test])), axis=1)\n",
    "scores_['loss_mae'] = reconstruction_errors\n",
    "\n",
    "scores_['Threshold'] = 0.75\n",
    "                 \n",
    "scores_['Anomaly'] = np.where(scores_[\"loss_mae\"] > scores_[\"Threshold\"], 1, 0)\n",
    "scores_.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores_ = pd.DataFrame()\n",
    "# scores_ = X_pred\n",
    "# scores_['datetime'] = df_timestamp.loc[227846:]\n",
    "# reconstruction_errors = np.mean(np.abs(X_pred - test), axis=1)\n",
    "# scores_['loss_mae'] = reconstruction_errors\n",
    "# scores_['Threshold'] = 0.75\n",
    "# scores_['Anomaly'] = np.where(scores_[\"loss_mae\"] > scores_[\"Threshold\"], 1, 0)\n",
    "# scores_.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error distribution i Test Data\n",
    "fig = go.Figure(data=[go.Histogram(x=scores_['loss_mae'])])\n",
    "fig.update_layout(title=\"Error distribution\", xaxis=dict(title=\"Loss Distribution between predicted and original Data of Credit Card\"), yaxis=dict(title=\"Data point counts\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_['Anomaly'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=scores_['datetime'],  y=scores_['loss_mae'], name=\"Loss\"))\n",
    "fig.add_trace(go.Scatter(x=scores_['datetime'],  y=scores_['Threshold'], name=\"Threshold\"))\n",
    "fig.update_layout(title=\"Error Time Series and Threshold\",  xaxis=dict(title=\"DateTime\"), yaxis=dict(title=\"Loss\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_[\"Anomaly\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = scores_[scores_['Anomaly'] == 1][['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12',\n",
    "                                              'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount','datetime']]\n",
    "\n",
    "anomalies = anomalies.rename(columns={\n",
    "    'V1': 'V1_anomalies',\n",
    "    'V2': 'V2_anomalies',\n",
    "    'V3': 'V3_anomalies',\n",
    "    'V4': 'V1_anomalies',\n",
    "    'V5': 'V5_anomalies',\n",
    "    'V6': 'V6_anomalies',\n",
    "    'V7': 'V7_anomalies',\n",
    "    'V8': 'V8_anomalies',\n",
    "    'V9': 'V9_anomalies',\n",
    "    'V10': 'V10_anomalies',\n",
    "    'V11': 'V11_anomalies',\n",
    "    'V12': 'V12_anomalies',\n",
    "    'V13': 'V13_anomalies',\n",
    "    'V14': 'V14_anomalies',\n",
    "    'V15': 'V15_anomalies',\n",
    "    'V16': 'V16_anomalies',\n",
    "    'V17': 'V17_anomalies',\n",
    "    'V18': 'V18_anomalies',\n",
    "    'V19': 'V19_anomalies',\n",
    "    'V20': 'V20_anomalies',\n",
    "    'V21': 'V21_anomalies',\n",
    "    'V22': 'V22_anomalies',\n",
    "    'V23': 'V23_anomalies',\n",
    "    'V24': 'V24_anomalies',\n",
    "    'V25': 'V25_anomalies',\n",
    "    'V26': 'V26_anomalies',\n",
    "    'V27': 'V27_anomalies',\n",
    "    'V28': 'V28_anomalies',\n",
    "    'Amount': 'Amount_anomalies'\n",
    "    \n",
    "})\n",
    "scores_1a = scores_.merge(anomalies, left_index=True, right_index=True, how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies = scores_[scores_['Anomaly'] == 1][[\n",
    "    'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12',\n",
    "                                              'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount', 'datetime']]\n",
    "print(anomalies.shape)\n",
    "anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_1a.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=scores_1a[\"datetime_x\"], y=scores_1a[\"V1\"], mode='lines', name='V1'))\n",
    "fig.add_trace(go.Scatter(x=scores_1a[\"datetime_x\"], y=scores_1a[\"V2\"], mode='lines', name='V2'))\n",
    "fig.update_layout(title_text=\"Test Data\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=scores_1a[\"datetime_x\"], y=scores_1a[\"V1\"], mode='lines', name='V1'))\n",
    "#fig.add_trace(go.Scatter(x=scores_1a[\"datetime_x\"], y=scores_1a[\"V1_anomalies\"], name='Anomaly in V1', mode='markers', marker=dict(color=\"blue\", size=11, line=dict(color=\"blue\", width=2))))\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter(x=scores_1a[\"datetime_x\"], y=scores_1a[\"V2\"], mode='lines', name='V2'))\n",
    "fig.add_trace(go.Scatter(x=scores_1a[\"datetime_x\"], y=scores_1a[\"V2_anomalies\"], name='Anomaly in V2 ', mode='markers', marker=dict(color=\"red\", size=11, line=dict(color=\"red\", width=2))))\n",
    "fig.update_layout(title_text=\"Anomalies Detected in V1 and V2 with LSTM-AE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Threshold columns\n",
    "df_anomaly = scores_.drop(\"Threshold\", axis=1)\n",
    "df_anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anomaly = df_anomaly.drop(\"loss_mae\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anomaly = df_anomaly.drop(\"Class\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anomaly.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_anomaly[\"Anomaly\"].value_counts()/284807)*100\n",
    "# Highly imbalanced dataset with 91% of data as not-fraud and only 8% of data as fraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_anomaly['Anomaly'] = np.where(df_anomaly['Anomaly'] == 0 ,'True', 'False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anomaly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Separate features and target variable\n",
    "# X = df_anomaly.drop('Anomaly', axis=1)\n",
    "# y = df_anomaly['Anomaly']\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Apply SMOTE only to the training data\n",
    "# smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "# X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# # Now, X_train_resampled and y_train_resampled contain the balanced training data\n",
    "# y_train_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only anomaly data \n",
    "df_anomaly.loc[df_anomaly['Anomaly']==1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random data generator 생성 함수 (데이터 증폭) \n",
    "def generate_random_data(df, dist=1, count=100):\n",
    "    '''\n",
    "    df: 대상 데이터 프레임 \n",
    "    dist: 원하는 mahalanobis distance 1, 0.5, 0.1, 0.01 \n",
    "    count: 개수\n",
    "    '''\n",
    "    # covarance metrix 구하기 \n",
    "    cov_matrix = np.cov(df, rowvar=False)\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "    # 마할라노비스 거리 스케일링 펙터 구하기 0.5, 0.1. 0.01  \n",
    "    scaling_factor = np.sqrt(dist/ eigenvalues.max())\n",
    "    scaled_cov_matrix = cov_matrix * scaling_factor\n",
    "    # print(scaled_cov_matrix)\n",
    "\n",
    "    random_data = np.random.multivariate_normal(df.mean(axis=0), scaled_cov_matrix, count)   #개수 만큼 생성 \n",
    "    print(f'generated data  : {random_data.shape},  count: {count} , dist: {dist}')\n",
    "    print('generated data 앞에서 3건만: ', random_data[:1, :3])\n",
    "    return random_data\n",
    "\n",
    "\n",
    "# temporary test for function \n",
    "generate_random_data(df_anomaly.loc[df_anomaly['Anomaly']==1, 'V1':'Amount'], 0.5, 100)\n",
    "generate_random_data(df_anomaly.loc[df_anomaly['Anomaly']==1, 'V1':'Amount'], 0.1, 100)\n",
    "generate_random_data(df_anomaly.loc[df_anomaly['Anomaly']==1, 'V1':'Amount'], 0.01, 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data preprocessing for LSTM\n",
    "- augmentation + sequence  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prp = .80\n",
    "\n",
    "df_anomaly_train = df_anomaly.loc[:df_anomaly.shape[0] * train_prp]\n",
    "df_anomaly_test = df_anomaly.loc[df_anomaly.shape[0] * train_prp:]\n",
    "\n",
    "print(f'train : {df_anomaly_train.shape}')\n",
    "print(f'test: {df_anomaly_test.shape}')\n",
    "print(df_anomaly_train.loc[df_anomaly_train['Anomaly']==1, :'datetime'].head())\n",
    "\n",
    "data_columns = anomalies.columns \n",
    "data_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TIMESTEPS = 12\n",
    "\n",
    "# 원본데이터(dataframe) + 증폭된 데이터 \n",
    "# concatenate orginal data and augumented data \n",
    "def train_data_with_concat(df, dist, count): \n",
    "    #X = df.drop(['time','class'],axis=1)\n",
    "    #데이터 만들기\n",
    "    X_concated = pd.concat([df.iloc[:, :-1], pd.DataFrame(generate_random_data(df.loc[df['Anomaly']==1, :'datetime'], dist, count), columns=data_columns)], ignore_index=True)\n",
    "    new_Y = pd.concat([df.loc[:, 'Anomaly'], pd.Series(np.ones((count,)).astype('bool'))], ignore_index=True)\n",
    "    # new_Y = pd.concat([result_df, pd.Series(np.ones((count,)).astype('bool'))], ignore_index=True)\n",
    "    return X_concated, new_Y \n",
    "\n",
    "# def train_array_with_concat(ar1, dist, count):\n",
    "#     X_concated = np.vstack((ar1, generate_random_data(ar1, dist, count)))\n",
    "#     X_concated = pd.DataFrame(X_concated, columns=X.columns)\n",
    "#     new_Y = pd.concat([df_anomaly.loc[:, 'Anomaly'], pd.Series(np.ones((count,)).astype('bool'))], ignore_index=True)\n",
    "#     return X_concated, new_Y \n",
    "\n",
    "# 시퀀스 데이터 생성 함수\n",
    "def create_sequences(df, y,  seq_length):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(df) - seq_length):\n",
    "        seq = df.iloc[i:i+seq_length].values\n",
    "        label = y.iloc[i+seq_length]\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "\n",
    "# 증폭된 데이터 만들기    0.1, 10000건 증폭 \n",
    "X_train, y_train = train_data_with_concat(df_anomaly_train, 0.1, 10000)\n",
    "X_test, y_test = train_data_with_concat(df_anomaly_test, 0.1, 10000)\n",
    "X_test_aug, y_test_aug = X_test, y_test \n",
    "print(f'증폭후 데이터:X_train:{X_train.shape},  y_train: {y_train.shape}, y_train :{np.bincount(y_train)} ') \n",
    "print(f'증폭후 데이터:X_test:{X_test.shape},  y_test: {y_test.shape}, y_test :{np.bincount(y_test)} ') \n",
    "# print(f'증폭후 데이터:X_test:{X_test.shape},  y_test: {y_test.shape}, y_test :{y_test} ') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# 언더샘플링 테스트코드 \n",
    "# x_resampled, y_resampled = RandomUnderSampler(random_state=0).fit_resample(X_train_origin, y_train_origin.astype('int'))\n",
    "x_resampled, y_resampled = RandomUnderSampler(random_state=0).fit_resample(X_test, y_test.astype('int'))\n",
    "y_resampled\n",
    "\n",
    "\n",
    "# 언더 샘플링하는 함수를 만들어보자 \n",
    "def make_undersample(x, y, sampling_strategy):\n",
    "    x_resampled, y_resampled =  RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=0).fit_resample(x, y)\n",
    "    return x_resampled, y_resampled \n",
    "    \n",
    "\n",
    "# make_undersample(X_test, y_test.astype('int'), {0:10000, 1:10000})\n",
    "# make_undersample(X_train_origin, y_train_origin.astype('int'), {0:100, 1:100})\n",
    "\n",
    "# 언더샘플링 적용 \n",
    "X_train, y_train = make_undersample(X_train, y_train.astype('int'),  {0:10000, 1:10000})        \n",
    "X_test, y_test = make_undersample(X_test, y_test.astype('int'),  {0:10000, 1:10000})        \n",
    "\n",
    "print(f'언더샘플링후 데이터: X_train: {X_train.shape} y_train: {y_train.shape}, y_test 0/1 개수:{np.bincount(y_train)} sample strategy:{ {0:10000, 1:10000}}') \n",
    "print(f'언더샘플링후 데이터: X_test: {X_test.shape} y_test: {y_test.shape}, y_test 0/1 개수:{np.bincount(y_test)} sample strategy:{ {0:10000, 1:10000}}') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시퀀스데이터 만들기 \n",
    "X_train, y_train = create_sequences(X_train, y_train, N_TIMESTEPS)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 30))\n",
    "y_train = to_categorical(y_train).astype(int)\n",
    "\n",
    "X_test, y_test = create_sequences(X_test, y_test, N_TIMESTEPS)\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 30))\n",
    "y_test = to_categorical(y_test).astype(int)\n",
    "\n",
    "# 증폭 후 데이터\n",
    "print(f'증폭후 데이터: X_train: {X_train.shape}, X_test:{X_test.shape} ') \n",
    "print(f'증폭후 데이터: y_train: {y_train.shape}, y_test:{y_test.shape} ') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본데이터로 시퀀스데이터 만들기(비교용, not augmentation)\n",
    "X_train_origin = df_anomaly_train.iloc[:, :-1]  \n",
    "X_test_origin = df_anomaly_test.iloc[:, :-1]  \n",
    "\n",
    "X_train_origin, y_train_origin  = create_sequences(X, y, 12)\n",
    "X_test_origin, y_test_origin  = create_sequences(X, y, 12)\n",
    "y_train_origin2 = to_categorical(y_train_origin).astype(int)\n",
    "y_test_origin2 = to_categorical(y_test_origin).astype(int)\n",
    "\n",
    "print(f'  증강안한 원본데이터를 시퀀스 형태로 변경후 X_train_origin: {X_train_origin.shape}, y_test_origin :{y_train_origin.shape}')\n",
    "print(f'  증강안한 원본데이터를 시퀀스 형태로 변경후 X_test_origin: {X_test_origin.shape}, y_test_origin :{y_test_origin.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the XAI LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm 모델 리턴하는 함수 \n",
    "def xai_lstm_model():\n",
    " \n",
    "    model2 = Sequential()\n",
    "    model2.add(LSTM(32, input_shape=(N_TIMESTEPS, len(data_columns))))\n",
    "    model2.add(Dropout(0.2))\n",
    "    model2.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=1e-4)\n",
    "    #model2.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    model2.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    #print(model2.summary())\n",
    "    return model2\n",
    "\n",
    "model2 = xai_lstm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 증강한 데이터로 학습 \n",
    "model2.fit(X_train, y_train, batch_size=100, epochs=100,\n",
    "                validation_data=(X_test,  y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(model2.predict(X_test), axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 증강 안한 데이터로 학습 \n",
    "\n",
    "model_no_aug = xai_lstm_model()\n",
    "model_no_aug.fit(X_train_origin, y_train_origin2, batch_size=100, epochs=10,\n",
    "                validation_data=(X_test_origin,  y_test_origin2), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = np.argmax(model_no_aug.predict(X_test_origin), axis=1)\n",
    "y_true2 = np.argmax(y_test_origin2, axis=1)\n",
    "\n",
    "print(classification_report(y_true2, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SMOTE로 oversampling 한 결과 모델링 및 결과. -> 이상함. 시계열데이터를 SMOTE기반으로 데이터 증강하는것은 이상함. \n",
    "\n",
    "# X_train_resampled, y_train_resampled \n",
    "\n",
    "# X_test_s, y_test_s = create_sequences(X_train_resampled, y_train_resampled, N_TIMESTEPS)\n",
    "# X_test_s = X_test_s.reshape((X_test_s.shape[0], X_test_s.shape[1], 30))\n",
    "# y_test_s2 = to_categorical(y_test_s).astype(int)\n",
    "\n",
    "# # 증폭 후 데이터\n",
    "# # print(f'증폭후 데이터: X_train: {X_train.shape}, X_test:{X_test.shape} ') \n",
    "\n",
    "# model_smote = xai_lstm_model()\n",
    "# model_smote.fit(X_test_s, y_test_s2, batch_size=100, epochs=10,\n",
    "#                 validation_data=(X_test_s,  y_test_s2), verbose=1)\n",
    "\n",
    "# y_pred_s = np.argmax(model_smote.predict(X_test_s), axis=1)\n",
    "# y_true_s = np.argmax(y_test_s2, axis=1)\n",
    "\n",
    "# print(classification_report(y_true_s, y_pred_s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lime(x, y, model, s): \n",
    "    explainer1 = lime_tabular.RecurrentTabularExplainer(x, training_labels=y, feature_names=data_columns,\n",
    "                                                    discretize_continuous=True,\n",
    "                                                    class_names=['False', 'True'],\n",
    "                                                    discretizer='decile')\n",
    "    exp = explainer1.explain_instance(x[0:1], model.predict, num_features=30, labels=(1,))\n",
    "    # exp.visualize_to_file('lime_explanation'+s+'.png')\n",
    "    fig = exp.as_pyplot_figure()\n",
    "    # fig.subplots_adjust(left=0.4, right=0.6)\n",
    "    fig.subplots_adjust(left=0.4)\n",
    "\n",
    "    fig.savefig('lime_explanation_credit'+s+'.png')\n",
    "    # plt.close(fig)\n",
    "    \n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".png\").name\n",
    "    exp.show_in_notebook()       \n",
    "    #image_result = exp.show_in_notebook()       \n",
    "    #image_result.savefig('lime_explanation'+s+'2.png')\n",
    "    plt.savefig(temp_file)\n",
    "    plt.close(fig)\n",
    "    Image(filename=temp_file)\n",
    "\n",
    "lime(X_test, y_test, model2, str(0.1)+\"_\"+str(1000)+\"_\"+str(1000) +\"_\"+ str(1000))  # XAI 코드 수정할 예정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no augmentation \n",
    "lime(X_test, y_test, model_no_aug, str(0.1)+\"_\"+str('no_aug')+\"_\"+str(1000) +\"_\"+ str(1000))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 첫 번째 그래프\n",
    "sns.kdeplot(data=df_anomaly.loc[df_anomaly['Anomaly'] == 0], x='Amount', label='Normal', fill=True)\n",
    "sns.kdeplot(data=df_anomaly.loc[df_anomaly['Anomaly'] == 1], x='Amount', label='Anomaly', fill=True)\n",
    "\n",
    "plt.title('Distribution of Sepal Length and Sepal Width')\n",
    "plt.xlabel('Measurement')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "# 그래프 보여주기\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anomaly.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anomaly.loc[:, :'Amount'].plot(kind='box', figsize=(10, 8), rot=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dicision boundry visualiation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# 가상의 시퀀스 데이터 생성\n",
    "# np.random.seed(42)\n",
    "seq_length = 12\n",
    "num_samples = 1000\n",
    "\n",
    "# X = np.random.rand(num_samples, seq_length, 1)\n",
    "# y = (np.sum(X, axis=1) > seq_length / 2).astype(int)\n",
    "\n",
    "# LSTM 이진 분류 모델 정의\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(10, input_shape=(seq_length, 1)))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "# model.fit(X, y, epochs=10, batch_size=32)\n",
    "\n",
    "# LSTM 모델의 출력을 얻기 위해 중간 레이어를 사용\n",
    "intermediate_layer_model = Sequential()\n",
    "intermediate_layer_model.add(LSTM(10, input_shape=(seq_length, 30), return_sequences=True))\n",
    "intermediate_layer_model.add(LSTM(10))  # 마지막 시간 스텝의 출력만 사용\n",
    "intermediate_output = intermediate_layer_model.predict(X_test)\n",
    "\n",
    "# 중간 레이어의 출력을 2D로 PCA를 사용하여 차원 축소\n",
    "pca = PCA(n_components=2)\n",
    "reduced_output = pca.fit_transform(intermediate_output)\n",
    "\n",
    "# Decision Boundary 시각화\n",
    "plt.scatter(reduced_output[:, 0], reduced_output[:, 1], c=np.argmax(y_test, axis=1), cmap='viridis')\n",
    "plt.title('Decision Boundary using PCA')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-SNE \n",
    "- aug 하기 전과 후의 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation 하기 전 결과 \n",
    "\n",
    "# LSTM의 마지막 레이어에서 나오는 특성을 추출\n",
    "layer_output = model2.layers[-2].output\n",
    "feature_extraction_model = Model(inputs=model2.input, outputs=layer_output)\n",
    "features = feature_extraction_model.predict(X_test_origin)\n",
    "\n",
    "# t-SNE를 사용하여 시각화\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "tsne_features = tsne.fit_transform(features)\n",
    "\n",
    "# 이진 분류 결과에 따라 다른 색으로 플로팅\n",
    "plt.scatter(tsne_features[:, 0], tsne_features[:, 1], c=y_test_origin, cmap=plt.cm.Spectral)\n",
    "plt.title(\"t-SNE Visualization of LSTM Decision Boundary without augmentation\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation 하고 나서의 결과 \n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "# LSTM의 마지막 레이어에서 나오는 특성을 추출\n",
    "layer_output = model2.layers[-2].output\n",
    "feature_extraction_model = Model(inputs=model2.input, outputs=layer_output)\n",
    "features = feature_extraction_model.predict(X_test)\n",
    "\n",
    "# t-SNE를 사용하여 시각화\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "tsne_features = tsne.fit_transform(features)\n",
    "\n",
    "# 이진 분류 결과에 따라 다른 색으로 플로팅\n",
    "plt.scatter(tsne_features[:, 0], tsne_features[:, 1], c=np.argmax(y_test, axis=1), cmap=plt.cm.Spectral)\n",
    "plt.title(\"t-SNE Visualization of LSTM Decision Boundary with aumentation data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data augmentation 하고 나서의 결과 \n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "# LSTM의 마지막 레이어에서 나오는 특성을 추출\n",
    "layer_output = model2.layers[-2].output\n",
    "feature_extraction_model = Model(inputs=model2.input, outputs=layer_output)\n",
    "features = feature_extraction_model.predict(X_test)\n",
    "\n",
    "# t-SNE를 사용하여 시각화\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "tsne_features = tsne.fit_transform(features)\n",
    "\n",
    "# 이진 분류 결과에 따라 다른 색으로 플로팅\n",
    "plt.scatter(tsne_features[:, 0], tsne_features[:, 1], c=np.argmax(y_test, axis=1), cmap=plt.cm.Spectral)\n",
    "plt.title(\"t-SNE Visualization of LSTM Decision Boundary with aumentation data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM의 마지막 레이어에서 나오는 특성을 추출 -> ERROR 남 \n",
    "layer_output = model.layers[-1].output\n",
    "feature_extraction_model = Model(inputs=model.input, outputs=layer_output)\n",
    "features = feature_extraction_model.predict(X_test_aug.values.reshape(X_test_aug.shape[0], 1, X_test_aug.shape[1]))\n",
    "print(features.shape)\n",
    "print(features.reshape(features[0], features[2]))\n",
    "\n",
    "# t-SNE를 사용하여 시각화\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "tsne_features = tsne.fit_transform(features.reshape(features[0], features[2]))\n",
    "\n",
    "# 이진 분류 결과에 따라 다른 색으로 플로팅\n",
    "plt.scatter(tsne_features[:, 0], tsne_features[:, 1], c=np.argmax(y_test, axis=1), cmap=plt.cm.Spectral)\n",
    "plt.title(\"t-SNE Visualization of LSTMAE Decision Boundary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_origin.reshape(X_test_origin.shape[0], 1, X_test_origin.shape[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOF 테스트해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOF example \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# 가상의 데이터 생성\n",
    "np.random.seed(42)\n",
    "X_train = np.random.normal(0, 1, (200, 2))\n",
    "X_outliers = np.random.uniform(low=-4, high=4, size=(10, 2))\n",
    "X_train = np.vstack([X_train, X_outliers])\n",
    "\n",
    "# LOF 모델 학습\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
    "y_pred = lof.fit_predict(X_train)\n",
    "\n",
    "# 이웃의 기여도를 표시하는 함수\n",
    "def show_neighbor_contributions(X, y_pred, lof_model):\n",
    "    for i, is_outlier in enumerate(y_pred == -1):  # 이상치인 경우만\n",
    "        if is_outlier:\n",
    "            instance = X[i].reshape(1, -1)\n",
    "            neighbors_idx = lof_model.kneighbors(instance, return_distance=False)[0]\n",
    "            \n",
    "            # 각 이웃의 기여도\n",
    "            neighbor_contributions = lof_model.negative_outlier_factor_[neighbors_idx]\n",
    "            \n",
    "            print(f\"이상치 데이터 포인트 {i+1}에 대한 이웃의 기여도:\")\n",
    "            for neighbor, contribution in zip(neighbors_idx, neighbor_contributions):\n",
    "                print(f\"이웃 {neighbor} - 기여도: {contribution:.4f}\")\n",
    "\n",
    "# 이웃의 기여도 및 이상치 시각화 함수\n",
    "def visualize_lof(X, y_pred, lof_model):\n",
    "    inliers = X[y_pred == 1]\n",
    "    outliers = X[y_pred == -1]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(inliers[:, 0], inliers[:, 1], color='green', label='Inliers')\n",
    "    plt.scatter(outliers[:, 0], outliers[:, 1], color='red', label='Outliers')\n",
    "    \n",
    "    plt.title('LOF anomal detection result 이상치 감지 결과')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # 이웃의 기여도 출력\n",
    "    show_neighbor_contributions(X, y_pred, lof_model)\n",
    "\n",
    "# 이상치 시각화 및 이웃의 기여도 출력\n",
    "visualize_lof(X_train, y_pred, lof)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anom_detect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
